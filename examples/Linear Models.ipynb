{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dojo.linear import (\n",
    "    LinearRegression,\n",
    "    LogisticRegression,\n",
    ")\n",
    "\n",
    "from dojo.split import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(\n",
       "    intercept=0,\n",
       "    coefs=[],\n",
       "    verbose=True,\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100_000, 100)\n",
    "y = X @ np.random.rand(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "Fitting...\n",
      "The model has been fitted successfully!\n",
      "-----------------------------------------\n",
      "CPU times: user 392 ms, sys: 108 ms, total: 500 ms\n",
      "Wall time: 309 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(\n",
       "    intercept=2.2260430217885263e-10,\n",
       "    coefs=[0.9945306  0.5165031  0.9059025  0.7753249  0.53530985 0.65397817\n",
       " 0.1727521  0.12112085 0.612782   0.70589304 0.4147605  0.25395572\n",
       " 0.8937312  0.8489611  0.91625667 0.8355145  0.16678327 0.47228104\n",
       " 0.86180955 0.59140915 0.41075975 0.71977806 0.10098992 0.5739429\n",
       " 0.29955992 0.67798764 0.24999176 0.33632833 0.16496293 0.4734186\n",
       " 0.09433009 0.7904253  0.00379142 0.2775983  0.23310497 0.32158145\n",
       " 0.0369086  0.7729702  0.3349079  0.83203626 0.67042685 0.74565977\n",
       " 0.3099707  0.75548446 0.11639068 0.6822007  0.053999   0.8799759\n",
       " 0.4615313  0.42833653 0.435901   0.4612403  0.15440509 0.05679084\n",
       " 0.26207104 0.8272514  0.16194251 0.88571435 0.8004571  0.91787994\n",
       " 0.3512158  0.20707574 0.40221894 0.01654747 0.15901947 0.0367812\n",
       " 0.9480005  0.1558629  0.24504234 0.8660024  0.42376766 0.96589285\n",
       " 0.45514038 0.17961068 0.95621586 0.9674539  0.2787239  0.29091665\n",
       " 0.08981159 0.94365937 0.68399227 0.27532357 0.46227533 0.6123534\n",
       " 0.83897084 0.9422925  0.69315416 0.8339859  0.83002824 0.11543981\n",
       " 0.46153215 0.33543327 0.5638538  0.9675281  0.6555131  0.885017\n",
       " 0.19171624 0.85195595 0.7477988  0.655899  ],\n",
       "    verbose=True,\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time linear_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.721124773909461e-13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_scores': array([2.94333381e-13, 2.94142415e-13, 2.94471784e-13, 2.93403705e-13,\n",
       "        2.93562237e-13, 2.94690146e-13, 2.93828397e-13, 2.94894843e-13,\n",
       "        2.94037650e-13, 2.94439564e-13]),\n",
       " 'test_scores': array([3.00172331e-13, 2.93625546e-13, 2.96928460e-13, 3.01697645e-13,\n",
       "        2.97173774e-13, 2.90013813e-13, 2.88403634e-13, 2.88171464e-13,\n",
       "        2.92826474e-13, 2.89434987e-13])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg.verbose = False\n",
    "cross_validate(linear_reg, X, y, k_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(alpha=7, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "    alpha=7,\n",
       "    loss=<dojo.losses.CrossEntropy object at 0x11951a5c0>,\n",
       "    regularizer=<dojo.regularizers.L2 object at 0x11951a630>,\n",
       "    verbose=True,\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [[1, 1],\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [0, 0]]\n",
    ")\n",
    "\n",
    "y = np.array([1 if x[0] and x[1] else 0 for x in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "10th iteration\n",
      "Loss: 0.8100873336041889\n",
      "--------------------------\n",
      "20th iteration\n",
      "Loss: 0.4254703074601587\n",
      "--------------------------\n",
      "30th iteration\n",
      "Loss: 0.3103356561023708\n",
      "--------------------------\n",
      "40th iteration\n",
      "Loss: 0.24595257483552338\n",
      "--------------------------\n",
      "50th iteration\n",
      "Loss: 0.2051483493686662\n",
      "--------------------------\n",
      "60th iteration\n",
      "Loss: 0.17715629334720134\n",
      "--------------------------\n",
      "70th iteration\n",
      "Loss: 0.15686739166536354\n",
      "--------------------------\n",
      "80th iteration\n",
      "Loss: 0.1415532452797914\n",
      "--------------------------\n",
      "90th iteration\n",
      "Loss: 0.1296285664579365\n",
      "--------------------------\n",
      "100th iteration\n",
      "Loss: 0.12011124600063036\n",
      "--------------------------\n",
      "110th iteration\n",
      "Loss: 0.11236169263465301\n",
      "--------------------------\n",
      "120th iteration\n",
      "Loss: 0.10594614072808313\n",
      "--------------------------\n",
      "130th iteration\n",
      "Loss: 0.10056039083145785\n",
      "--------------------------\n",
      "140th iteration\n",
      "Loss: 0.09598509766304131\n",
      "--------------------------\n",
      "150th iteration\n",
      "Loss: 0.09205824206884602\n",
      "--------------------------\n",
      "160th iteration\n",
      "Loss: 0.08865767839698682\n",
      "--------------------------\n",
      "170th iteration\n",
      "Loss: 0.08568966819686784\n",
      "--------------------------\n",
      "180th iteration\n",
      "Loss: 0.0830811046573763\n",
      "--------------------------\n",
      "190th iteration\n",
      "Loss: 0.08077424129467438\n",
      "--------------------------\n",
      "200th iteration\n",
      "Loss: 0.0787228056073864\n",
      "--------------------------\n",
      "210th iteration\n",
      "Loss: 0.07688935477943692\n",
      "--------------------------\n",
      "220th iteration\n",
      "Loss: 0.07524326349385138\n",
      "--------------------------\n",
      "230th iteration\n",
      "Loss: 0.07375928876021094\n",
      "--------------------------\n",
      "240th iteration\n",
      "Loss: 0.07241640248891967\n",
      "--------------------------\n",
      "250th iteration\n",
      "Loss: 0.07119699164205025\n",
      "--------------------------\n",
      "260th iteration\n",
      "Loss: 0.07008616803281087\n",
      "--------------------------\n",
      "270th iteration\n",
      "Loss: 0.06907130496860638\n",
      "--------------------------\n",
      "280th iteration\n",
      "Loss: 0.06814159709020587\n",
      "--------------------------\n",
      "290th iteration\n",
      "Loss: 0.06728776600863327\n",
      "--------------------------\n",
      "300th iteration\n",
      "Loss: 0.06650178693054896\n",
      "--------------------------\n",
      "310th iteration\n",
      "Loss: 0.0657766717897817\n",
      "--------------------------\n",
      "320th iteration\n",
      "Loss: 0.065106395942652\n",
      "--------------------------\n",
      "330th iteration\n",
      "Loss: 0.06448562133930032\n",
      "--------------------------\n",
      "340th iteration\n",
      "Loss: 0.0639096666319951\n",
      "--------------------------\n",
      "350th iteration\n",
      "Loss: 0.06337440735246333\n",
      "--------------------------\n",
      "360th iteration\n",
      "Loss: 0.06287618675946674\n",
      "--------------------------\n",
      "370th iteration\n",
      "Loss: 0.06241178480437602\n",
      "--------------------------\n",
      "380th iteration\n",
      "Loss: 0.06197830383466236\n",
      "--------------------------\n",
      "390th iteration\n",
      "Loss: 0.06157316475715701\n",
      "--------------------------\n",
      "400th iteration\n",
      "Loss: 0.061194010670691024\n",
      "--------------------------\n",
      "410th iteration\n",
      "Loss: 0.060838819067958926\n",
      "--------------------------\n",
      "420th iteration\n",
      "Loss: 0.06050567323169769\n",
      "--------------------------\n",
      "430th iteration\n",
      "Loss: 0.06019289356342367\n",
      "--------------------------\n",
      "440th iteration\n",
      "Loss: 0.05989896180517693\n",
      "--------------------------\n",
      "450th iteration\n",
      "Loss: 0.05962248330620362\n",
      "--------------------------\n",
      "460th iteration\n",
      "Loss: 0.059362171286419546\n",
      "--------------------------\n",
      "470th iteration\n",
      "Loss: 0.05911689906112165\n",
      "--------------------------\n",
      "480th iteration\n",
      "Loss: 0.05888563173190804\n",
      "--------------------------\n",
      "490th iteration\n",
      "Loss: 0.05866738098129989\n",
      "--------------------------\n",
      "500th iteration\n",
      "Loss: 0.05846126181563638\n",
      "--------------------------\n",
      "510th iteration\n",
      "Loss: 0.05826649224031204\n",
      "--------------------------\n",
      "520th iteration\n",
      "Loss: 0.05808230940987434\n",
      "--------------------------\n",
      "530th iteration\n",
      "Loss: 0.057908038919676996\n",
      "--------------------------\n",
      "540th iteration\n",
      "Loss: 0.05774305710810104\n",
      "--------------------------\n",
      "550th iteration\n",
      "Loss: 0.05758679879139831\n",
      "--------------------------\n",
      "560th iteration\n",
      "Loss: 0.05743867003330612\n",
      "--------------------------\n",
      "570th iteration\n",
      "Loss: 0.05729821920941567\n",
      "--------------------------\n",
      "580th iteration\n",
      "Loss: 0.05716498353882561\n",
      "--------------------------\n",
      "590th iteration\n",
      "Loss: 0.057038517880084474\n",
      "--------------------------\n",
      "600th iteration\n",
      "Loss: 0.056918443212249786\n",
      "--------------------------\n",
      "610th iteration\n",
      "Loss: 0.05680437962883018\n",
      "--------------------------\n",
      "620th iteration\n",
      "Loss: 0.05669598795696944\n",
      "--------------------------\n",
      "630th iteration\n",
      "Loss: 0.05659294638256534\n",
      "--------------------------\n",
      "640th iteration\n",
      "Loss: 0.05649495261384169\n",
      "--------------------------\n",
      "650th iteration\n",
      "Loss: 0.056401728626117434\n",
      "--------------------------\n",
      "660th iteration\n",
      "Loss: 0.05631300949080943\n",
      "--------------------------\n",
      "670th iteration\n",
      "Loss: 0.056228574455269964\n",
      "--------------------------\n",
      "680th iteration\n",
      "Loss: 0.05614814750230272\n",
      "--------------------------\n",
      "690th iteration\n",
      "Loss: 0.05607155451747285\n",
      "--------------------------\n",
      "700th iteration\n",
      "Loss: 0.055998569974448964\n",
      "--------------------------\n",
      "710th iteration\n",
      "Loss: 0.055929030367509354\n",
      "--------------------------\n",
      "720th iteration\n",
      "Loss: 0.05586274545776686\n",
      "--------------------------\n",
      "730th iteration\n",
      "Loss: 0.05579954618132249\n",
      "--------------------------\n",
      "740th iteration\n",
      "Loss: 0.0557392861216831\n",
      "--------------------------\n",
      "750th iteration\n",
      "Loss: 0.05568176667821177\n",
      "--------------------------\n",
      "760th iteration\n",
      "Loss: 0.05562689580248283\n",
      "--------------------------\n",
      "770th iteration\n",
      "Loss: 0.05557454890910561\n",
      "--------------------------\n",
      "780th iteration\n",
      "Loss: 0.055524583853944556\n",
      "--------------------------\n",
      "790th iteration\n",
      "Loss: 0.05547688084401391\n",
      "--------------------------\n",
      "800th iteration\n",
      "Loss: 0.05543135493289622\n",
      "--------------------------\n",
      "810th iteration\n",
      "Loss: 0.055387861987635637\n",
      "--------------------------\n",
      "820th iteration\n",
      "Loss: 0.05534632125086966\n",
      "--------------------------\n",
      "830th iteration\n",
      "Loss: 0.05530664253007968\n",
      "--------------------------\n",
      "840th iteration\n",
      "Loss: 0.05526872534584645\n",
      "--------------------------\n",
      "850th iteration\n",
      "Loss: 0.055232502075290166\n",
      "--------------------------\n",
      "860th iteration\n",
      "Loss: 0.0551978766135762\n",
      "--------------------------\n",
      "870th iteration\n",
      "Loss: 0.05516478714303963\n",
      "--------------------------\n",
      "880th iteration\n",
      "Loss: 0.05513318563349716\n",
      "--------------------------\n",
      "890th iteration\n",
      "Loss: 0.05510295039403777\n",
      "--------------------------\n",
      "900th iteration\n",
      "Loss: 0.05507405914065108\n",
      "--------------------------\n",
      "910th iteration\n",
      "Loss: 0.05504637869326598\n",
      "--------------------------\n",
      "920th iteration\n",
      "Loss: 0.055019941860762436\n",
      "--------------------------\n",
      "930th iteration\n",
      "Loss: 0.05499465383936744\n",
      "--------------------------\n",
      "940th iteration\n",
      "Loss: 0.05497047058459643\n",
      "--------------------------\n",
      "950th iteration\n",
      "Loss: 0.05494733529119828\n",
      "--------------------------\n",
      "960th iteration\n",
      "Loss: 0.05492520659173364\n",
      "--------------------------\n",
      "970th iteration\n",
      "Loss: 0.05490403223978087\n",
      "--------------------------\n",
      "980th iteration\n",
      "Loss: 0.05488376582192199\n",
      "--------------------------\n",
      "990th iteration\n",
      "Loss: 0.05486438182278827\n",
      "--------------------------\n",
      "1000th iteration\n",
      "Loss: 0.05484582545929215\n",
      "--------------------------\n",
      "1010th iteration\n",
      "Loss: 0.054828054810611135\n",
      "--------------------------\n",
      "1020th iteration\n",
      "Loss: 0.05481106923975828\n",
      "--------------------------\n",
      "1030th iteration\n",
      "Loss: 0.05479480366891326\n",
      "--------------------------\n",
      "1040th iteration\n",
      "Loss: 0.054779227874027295\n",
      "--------------------------\n",
      "1050th iteration\n",
      "Loss: 0.05476431784436959\n",
      "--------------------------\n",
      "1060th iteration\n",
      "Loss: 0.054750041991753796\n",
      "--------------------------\n",
      "1070th iteration\n",
      "Loss: 0.05473638963375188\n",
      "--------------------------\n",
      "1080th iteration\n",
      "Loss: 0.05472330926859814\n",
      "--------------------------\n",
      "1090th iteration\n",
      "Loss: 0.0547107828252025\n",
      "--------------------------\n",
      "1100th iteration\n",
      "Loss: 0.05469879971831614\n",
      "--------------------------\n",
      "1110th iteration\n",
      "Loss: 0.05468730904910686\n",
      "--------------------------\n",
      "1120th iteration\n",
      "Loss: 0.054676318839944386\n",
      "--------------------------\n",
      "1130th iteration\n",
      "Loss: 0.054665793062343024\n",
      "--------------------------\n",
      "1140th iteration\n",
      "Loss: 0.054655704903439566\n",
      "--------------------------\n",
      "1150th iteration\n",
      "Loss: 0.054646040688563303\n",
      "--------------------------\n",
      "1160th iteration\n",
      "Loss: 0.05463678667137055\n",
      "--------------------------\n",
      "1170th iteration\n",
      "Loss: 0.05462792917075396\n",
      "--------------------------\n",
      "1180th iteration\n",
      "Loss: 0.054619445618734885\n",
      "--------------------------\n",
      "1190th iteration\n",
      "Loss: 0.05461131862223412\n",
      "--------------------------\n",
      "1200th iteration\n",
      "Loss: 0.05460353661515967\n",
      "--------------------------\n",
      "1210th iteration\n",
      "Loss: 0.05459607515181319\n",
      "--------------------------\n",
      "1220th iteration\n",
      "Loss: 0.05458893185916512\n",
      "--------------------------\n",
      "1230th iteration\n",
      "Loss: 0.054582080508656414\n",
      "--------------------------\n",
      "1240th iteration\n",
      "Loss: 0.05457552455563466\n",
      "--------------------------\n",
      "1250th iteration\n",
      "Loss: 0.05456924879691376\n",
      "--------------------------\n",
      "1260th iteration\n",
      "Loss: 0.05456322705290498\n",
      "--------------------------\n",
      "1270th iteration\n",
      "Loss: 0.05455745900693047\n",
      "--------------------------\n",
      "1280th iteration\n",
      "Loss: 0.054551946344109876\n",
      "--------------------------\n",
      "1290th iteration\n",
      "Loss: 0.0545466481737706\n",
      "--------------------------\n",
      "1300th iteration\n",
      "Loss: 0.05454157905904802\n",
      "--------------------------\n",
      "1310th iteration\n",
      "Loss: 0.05453672588222416\n",
      "--------------------------\n",
      "1320th iteration\n",
      "Loss: 0.05453206260310279\n",
      "--------------------------\n",
      "1330th iteration\n",
      "Loss: 0.05452760392460888\n",
      "--------------------------\n",
      "1340th iteration\n",
      "Loss: 0.0545233328719725\n",
      "--------------------------\n",
      "1350th iteration\n",
      "Loss: 0.05451924028751652\n",
      "--------------------------\n",
      "1360th iteration\n",
      "Loss: 0.05451531118899926\n",
      "--------------------------\n",
      "1370th iteration\n",
      "Loss: 0.054511560235461304\n",
      "--------------------------\n",
      "1380th iteration\n",
      "Loss: 0.0545079615226985\n",
      "--------------------------\n",
      "1390th iteration\n",
      "Loss: 0.05450451305577473\n",
      "--------------------------\n",
      "1400th iteration\n",
      "Loss: 0.054501205619876515\n",
      "--------------------------\n",
      "1410th iteration\n",
      "Loss: 0.05449803532758667\n",
      "--------------------------\n",
      "1420th iteration\n",
      "Loss: 0.05449499107564294\n",
      "--------------------------\n",
      "1430th iteration\n",
      "Loss: 0.05449208761240312\n",
      "--------------------------\n",
      "1440th iteration\n",
      "Loss: 0.054489295263443586\n",
      "--------------------------\n",
      "1450th iteration\n",
      "Loss: 0.054486617755500524\n",
      "--------------------------\n",
      "1460th iteration\n",
      "Loss: 0.05448405505868363\n",
      "--------------------------\n",
      "1470th iteration\n",
      "Loss: 0.054481603279669805\n",
      "--------------------------\n",
      "1480th iteration\n",
      "Loss: 0.0544792423033401\n",
      "--------------------------\n",
      "1490th iteration\n",
      "Loss: 0.05447698119143163\n",
      "--------------------------\n",
      "1500th iteration\n",
      "Loss: 0.05447483276122679\n",
      "--------------------------\n",
      "1510th iteration\n",
      "Loss: 0.05447276019737861\n",
      "--------------------------\n",
      "1520th iteration\n",
      "Loss: 0.05447076347287813\n",
      "--------------------------\n",
      "1530th iteration\n",
      "Loss: 0.05446887935527908\n",
      "--------------------------\n",
      "1540th iteration\n",
      "Loss: 0.05446705810357492\n",
      "--------------------------\n",
      "1550th iteration\n",
      "Loss: 0.054465301628313575\n",
      "--------------------------\n",
      "1560th iteration\n",
      "Loss: 0.054463644745747025\n",
      "--------------------------\n",
      "1570th iteration\n",
      "Loss: 0.05446201198568515\n",
      "--------------------------\n",
      "1580th iteration\n",
      "Loss: 0.0544605064892022\n",
      "--------------------------\n",
      "1590th iteration\n",
      "Loss: 0.054458999284453406\n",
      "--------------------------\n",
      "1600th iteration\n",
      "Loss: 0.05445761929962138\n",
      "--------------------------\n",
      "1610th iteration\n",
      "Loss: 0.054456239488497524\n",
      "--------------------------\n",
      "1620th iteration\n",
      "Loss: 0.05445497204970488\n",
      "--------------------------\n",
      "1630th iteration\n",
      "Loss: 0.054453717656219855\n",
      "--------------------------\n",
      "1640th iteration\n",
      "Loss: 0.05445252593945361\n",
      "--------------------------\n",
      "1650th iteration\n",
      "Loss: 0.054451396920362514\n",
      "--------------------------\n",
      "1660th iteration\n",
      "Loss: 0.05445026799692975\n",
      "--------------------------\n",
      "1670th iteration\n",
      "Loss: 0.05444925139312437\n",
      "--------------------------\n",
      "1680th iteration\n",
      "Loss: 0.05444824778405722\n",
      "--------------------------\n",
      "1690th iteration\n",
      "Loss: 0.05444725714437748\n",
      "--------------------------\n",
      "1700th iteration\n",
      "Loss: 0.05444636590203397\n",
      "--------------------------\n",
      "1710th iteration\n",
      "Loss: 0.05444548763546565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "1720th iteration\n",
      "Loss: 0.05444461133306834\n",
      "--------------------------\n",
      "1730th iteration\n",
      "Loss: 0.05444380861650933\n",
      "--------------------------\n",
      "1740th iteration\n",
      "Loss: 0.054443055626975295\n",
      "--------------------------\n",
      "1750th iteration\n",
      "Loss: 0.05444230268746353\n",
      "--------------------------\n",
      "1760th iteration\n",
      "Loss: 0.054441551698601015\n",
      "--------------------------\n",
      "1770th iteration\n",
      "Loss: 0.05444087427852204\n",
      "--------------------------\n",
      "1780th iteration\n",
      "Loss: 0.054440246558749975\n",
      "--------------------------\n",
      "1790th iteration\n",
      "Loss: 0.05443962078459716\n",
      "--------------------------\n",
      "1800th iteration\n",
      "Loss: 0.05443899313542416\n",
      "--------------------------\n",
      "1810th iteration\n",
      "Loss: 0.054438391289387954\n",
      "--------------------------\n",
      "1820th iteration\n",
      "Loss: 0.0544378649076655\n",
      "--------------------------\n",
      "1830th iteration\n",
      "Loss: 0.054437351445662766\n",
      "--------------------------\n",
      "1840th iteration\n",
      "Loss: 0.05443685090411422\n",
      "--------------------------\n",
      "1850th iteration\n",
      "Loss: 0.054436348468100984\n",
      "--------------------------\n",
      "1860th iteration\n",
      "Loss: 0.05443584795854341\n",
      "--------------------------\n",
      "1870th iteration\n",
      "Loss: 0.054435397116768064\n",
      "--------------------------\n",
      "1880th iteration\n",
      "Loss: 0.0544349701654747\n",
      "--------------------------\n",
      "1890th iteration\n",
      "Loss: 0.054434581899618625\n",
      "--------------------------\n",
      "1900th iteration\n",
      "Loss: 0.054434204634388875\n",
      "--------------------------\n",
      "1910th iteration\n",
      "Loss: 0.05443382928889047\n",
      "--------------------------\n",
      "1920th iteration\n",
      "Loss: 0.05443345395297833\n",
      "--------------------------\n",
      "1930th iteration\n",
      "Loss: 0.05443307671683041\n",
      "--------------------------\n",
      "1940th iteration\n",
      "Loss: 0.05443271428598785\n",
      "--------------------------\n",
      "1950th iteration\n",
      "Loss: 0.054432401512873575\n",
      "--------------------------\n",
      "1960th iteration\n",
      "Loss: 0.054432099732882946\n",
      "--------------------------\n",
      "1970th iteration\n",
      "Loss: 0.05443181275766322\n",
      "--------------------------\n",
      "1980th iteration\n",
      "Loss: 0.05443156064346932\n",
      "--------------------------\n",
      "1990th iteration\n",
      "Loss: 0.054431310443784894\n",
      "--------------------------\n",
      "2000th iteration\n",
      "Loss: 0.054431060250685996\n",
      "--------------------------\n",
      "2010th iteration\n",
      "Loss: 0.05443081006076661\n",
      "--------------------------\n",
      "2020th iteration\n",
      "Loss: 0.054430557968233935\n",
      "--------------------------\n",
      "2030th iteration\n",
      "Loss: 0.05443030778619526\n",
      "--------------------------\n",
      "2040th iteration\n",
      "Loss: 0.05443005760730184\n",
      "--------------------------\n",
      "2050th iteration\n",
      "Loss: 0.05442982031942006\n",
      "--------------------------\n",
      "2060th iteration\n",
      "Loss: 0.05442961979366745\n",
      "--------------------------\n",
      "2070th iteration\n",
      "Loss: 0.05442941927303564\n",
      "--------------------------\n",
      "2080th iteration\n",
      "Loss: 0.054429242624362606\n",
      "--------------------------\n",
      "2090th iteration\n",
      "Loss: 0.05442908077534969\n",
      "--------------------------\n",
      "2100th iteration\n",
      "Loss: 0.05442891702273274\n",
      "--------------------------\n",
      "2110th iteration\n",
      "Loss: 0.05442876616054465\n",
      "--------------------------\n",
      "2120th iteration\n",
      "Loss: 0.05442863009686277\n",
      "--------------------------\n",
      "2130th iteration\n",
      "Loss: 0.054428503108425424\n",
      "--------------------------\n",
      "2140th iteration\n",
      "Loss: 0.05442837802907191\n",
      "--------------------------\n",
      "2150th iteration\n",
      "Loss: 0.05442825295053702\n",
      "--------------------------\n",
      "2160th iteration\n",
      "Loss: 0.05442812787360915\n",
      "--------------------------\n",
      "2170th iteration\n",
      "Loss: 0.054428002796680544\n",
      "--------------------------\n",
      "2180th iteration\n",
      "Loss: 0.05442787772132565\n",
      "--------------------------\n",
      "2190th iteration\n",
      "Loss: 0.05442775264675709\n",
      "--------------------------\n",
      "2200th iteration\n",
      "Loss: 0.05442762757375888\n",
      "--------------------------\n",
      "2210th iteration\n",
      "Loss: 0.05442750250076213\n",
      "--------------------------\n",
      "2220th iteration\n",
      "Loss: 0.05442737552041945\n",
      "--------------------------\n",
      "2230th iteration\n",
      "Loss: 0.05442725044899568\n",
      "--------------------------\n",
      "2240th iteration\n",
      "Loss: 0.05442712537835824\n",
      "--------------------------\n",
      "2250th iteration\n",
      "Loss: 0.0544270131965475\n",
      "--------------------------\n",
      "2260th iteration\n",
      "Loss: 0.05442691199532529\n",
      "--------------------------\n",
      "2270th iteration\n",
      "Loss: 0.05442682559007689\n",
      "--------------------------\n",
      "2280th iteration\n",
      "Loss: 0.0544267372786078\n",
      "--------------------------\n",
      "2290th iteration\n",
      "Loss: 0.05442664896812356\n",
      "--------------------------\n",
      "2300th iteration\n",
      "Loss: 0.0544265754532658\n",
      "--------------------------\n",
      "2310th iteration\n",
      "Loss: 0.054426500031374656\n",
      "--------------------------\n",
      "2320th iteration\n",
      "Loss: 0.05442643749763616\n",
      "--------------------------\n",
      "2330th iteration\n",
      "Loss: 0.05442636207676011\n",
      "CPU times: user 597 ms, sys: 46.2 ms, total: 644 ms\n",
      "Wall time: 617 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "    alpha=7,\n",
       "    loss=<dojo.losses.CrossEntropy object at 0x11951a5c0>,\n",
       "    regularizer=<dojo.regularizers.L2 object at 0x11951a630>,\n",
       "    verbose=True,\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
